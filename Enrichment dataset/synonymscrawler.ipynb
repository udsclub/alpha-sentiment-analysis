{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from synonymscrawler import simple_synonyms_crawler\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim import models\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '../'\n",
    "EMBEDDING_DIR = BASE_DIR + 'alpha/embeddings/'  # http://nlp.stanford.edu/projects/glove/ pretrained vectors\n",
    "EMBEDDING_FILE = \"GoogleNews-vectors-negative300.bin\"\n",
    "TEXT_DATA_DIR = BASE_DIR + 'data/'\n",
    "TEXT_DATA_FILE = \"movie_reviews.csv\"\n",
    "HEADER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(os.path.join(TEXT_DATA_DIR, TEXT_DATA_FILE), \"r\") as f:\n",
    "        if HEADER:\n",
    "            _ = next(f)\n",
    "        for line in f:\n",
    "            temp_y, temp_x = line.rstrip(\"\\n\").split(\",\", 1)\n",
    "            x.append(temp_x)\n",
    "            y.append(temp_y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_data()\n",
    "y = np.array(y, dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='\"Â«#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.!?_[1-9]')\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeDict(d, filename, sep=\";\"):\n",
    "    with open(filename, \"a\") as f:\n",
    "        for i in d.keys():            \n",
    "            f.write(i + sep + sep.join([str(x) for x in d[i]]) + \"\\n\")\n",
    "            \n",
    "def readDict(filename, sep=\";\"):\n",
    "    with open(filename, \"r\") as f:\n",
    "        dict = {}\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(sep)\n",
    "            dict[values[0]] = values[1:]\n",
    "        return(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_synonyms(x):\n",
    "    try:\n",
    "        return simple_synonyms_crawler.crawl(x, 3)\n",
    "    except:\n",
    "        return [x, x, x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "arr = list(range(0,len(tokenizer.word_index.keys()),10000)) + [len(tokenizer.word_index.keys())]\n",
    "for n in range(len(arr)-1):\n",
    "    print(n)\n",
    "    d = {}\n",
    "    pool = ThreadPool(16)\n",
    "    results = pool.map(lambda w: get_synonyms(w), list(tokenizer.word_index.keys())[arr[n]:arr[n+1]])\n",
    "    for i in results:\n",
    "        d[i[0]] = i[1:]\n",
    "    writeDict(d, \"synonyms.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arr)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = readDict(\"synonyms.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'satisfying']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
